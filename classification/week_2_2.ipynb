{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from datetime import datetime, timedelta,date\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv('./../data/amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/important_words.json', 'r') as f: # Reads the list of most frequent words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of positive reviews = 26579\n",
      "# of negative reviews = 26493\n"
     ]
    }
   ],
   "source": [
    "print('# of positive reviews =', len(products_df[products_df['sentiment']==1]))\n",
    "print('# of negative reviews =', len(products_df[products_df['sentiment']==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.review = products_df.review.fillna('')\n",
    "products_df['reviews_clean'] = products_df.review.str.replace('[^\\w\\s]','').fillna('')\n",
    "for word in important_words:\n",
    "    products_df[word] = products_df['reviews_clean'].apply(lambda s : s.split().count(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes = pd.read_json('../data/module-4-assignment-train-idx.json')\n",
    "val_indexes = pd.read_json('../data/module-4-assignment-validation-idx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = products_df.iloc[train_indexes[0].tolist()]\n",
    "val_data = products_df.iloc[val_indexes[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_frame, features, label):\n",
    "    \"\"\"\n",
    "    features: list of feature columns\n",
    "    label: str label columns name\n",
    "    \"\"\"\n",
    "    data_frame['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_frame = data_frame[features]\n",
    "    feature_matrix = features_frame.as_matrix()\n",
    "    label_frame  = data_frame[label]\n",
    "    label_array = label_frame.as_matrix()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nehiljain/.pyenv/versions/3.4.6/envs/py34/lib/python3.4/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(val_data, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    # YOUR CODE HERE\n",
    "    scores = feature_matrix.dot(coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1.0/(1.0 + np.exp(-scores))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "    derivative = np.dot(errors, feature)\n",
    "    if not feature_is_constant:\n",
    "        derivative = np.dot(errors, feature) - 2 * l2_penalty * coefficient\n",
    "    return derivative\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in \n",
    "# run with L2 = 10\n",
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=10, max_iter=501)range(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative_with_L2(errors, feature_matrix[:, j], coefficients[j], l2_penalty, is_intercept)\n",
    "            \n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients = coefficients + step_size * derivative\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -38432.40613172\n",
      "iteration   1: log likelihood of observed labels = -347079.20583932\n",
      "iteration   2: log likelihood of observed labels = -270977.08597932\n",
      "iteration   3: log likelihood of observed labels = -292539.75364679\n",
      "iteration   4: log likelihood of observed labels = -320156.20846519\n",
      "iteration   5: log likelihood of observed labels = -239770.07077246\n",
      "iteration   6: log likelihood of observed labels = -367049.81808239\n",
      "iteration   7: log likelihood of observed labels = -189344.44520061\n",
      "iteration   8: log likelihood of observed labels = -410170.31294279\n",
      "iteration   9: log likelihood of observed labels = -143256.74334393\n",
      "iteration  10: log likelihood of observed labels = -445576.99758584\n",
      "iteration  11: log likelihood of observed labels = -106062.47829825\n",
      "iteration  12: log likelihood of observed labels = -466035.54319332\n",
      "iteration  13: log likelihood of observed labels = -85177.65127790\n",
      "iteration  14: log likelihood of observed labels = -468985.63336584\n",
      "iteration  15: log likelihood of observed labels = -82227.14294709\n",
      "iteration  20: log likelihood of observed labels = -468533.98541168\n",
      "iteration  30: log likelihood of observed labels = -468539.16317204\n",
      "iteration  40: log likelihood of observed labels = -468539.16025058\n",
      "iteration  50: log likelihood of observed labels = -468539.16025223\n",
      "iteration  60: log likelihood of observed labels = -468539.16025223\n",
      "iteration  70: log likelihood of observed labels = -468539.16025223\n",
      "iteration  80: log likelihood of observed labels = -468539.16025223\n",
      "iteration  90: log likelihood of observed labels = -468539.16025223\n",
      "iteration 100: log likelihood of observed labels = -468539.16025223\n",
      "iteration 200: log likelihood of observed labels = -468539.16025223\n",
      "iteration 300: log likelihood of observed labels = -468539.16025223\n",
      "iteration 400: log likelihood of observed labels = -468539.16025223\n",
      "iteration 500: log likelihood of observed labels = -468539.16025223\n"
     ]
    }
   ],
   "source": [
    "coefficients_0_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=0, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -38425.25480580\n",
      "iteration   1: log likelihood of observed labels = -344048.39077454\n",
      "iteration   2: log likelihood of observed labels = -275279.32666684\n",
      "iteration   3: log likelihood of observed labels = -288357.94233088\n",
      "iteration   4: log likelihood of observed labels = -324547.86231617\n",
      "iteration   5: log likelihood of observed labels = -236637.14234075\n",
      "iteration   6: log likelihood of observed labels = -369727.58802478\n",
      "iteration   7: log likelihood of observed labels = -189270.02727123\n",
      "iteration   8: log likelihood of observed labels = -409634.66047233\n",
      "iteration   9: log likelihood of observed labels = -147772.38847181\n",
      "iteration  10: log likelihood of observed labels = -441446.06015742\n",
      "iteration  11: log likelihood of observed labels = -115211.99034563\n",
      "iteration  12: log likelihood of observed labels = -460995.01246537\n",
      "iteration  13: log likelihood of observed labels = -95637.72003658\n",
      "iteration  14: log likelihood of observed labels = -467350.59290292\n",
      "iteration  15: log likelihood of observed labels = -89385.15301003\n",
      "iteration  20: log likelihood of observed labels = -467798.62872915\n",
      "iteration  30: log likelihood of observed labels = -467798.62510644\n",
      "iteration  40: log likelihood of observed labels = -467798.62510644\n",
      "iteration  50: log likelihood of observed labels = -467798.62510644\n",
      "iteration  60: log likelihood of observed labels = -467798.62510644\n",
      "iteration  70: log likelihood of observed labels = -467798.62510644\n",
      "iteration  80: log likelihood of observed labels = -467798.62510644\n",
      "iteration  90: log likelihood of observed labels = -467798.62510644\n",
      "iteration 100: log likelihood of observed labels = -467798.62510644\n",
      "iteration 200: log likelihood of observed labels = -467798.62510644\n",
      "iteration 300: log likelihood of observed labels = -467798.62510644\n",
      "iteration 400: log likelihood of observed labels = -467798.62510644\n",
      "iteration 500: log likelihood of observed labels = -467798.62510644\n",
      "iteration   0: log likelihood of observed labels = -38313.79412959\n",
      "iteration   1: log likelihood of observed labels = -315639.03766911\n",
      "iteration   2: log likelihood of observed labels = -306610.21409835\n",
      "iteration   3: log likelihood of observed labels = -261646.59339013\n",
      "iteration   4: log likelihood of observed labels = -347269.03275182\n",
      "iteration   5: log likelihood of observed labels = -226969.72458648\n",
      "iteration   6: log likelihood of observed labels = -373346.49273633\n",
      "iteration   7: log likelihood of observed labels = -205078.84147888\n",
      "iteration   8: log likelihood of observed labels = -389576.33599274\n",
      "iteration   9: log likelihood of observed labels = -191609.46850498\n",
      "iteration  10: log likelihood of observed labels = -399373.53948374\n",
      "iteration  11: log likelihood of observed labels = -183539.98944596\n",
      "iteration  12: log likelihood of observed labels = -405139.30407667\n",
      "iteration  13: log likelihood of observed labels = -178813.49568034\n",
      "iteration  14: log likelihood of observed labels = -408470.61538902\n",
      "iteration  15: log likelihood of observed labels = -176090.43655159\n",
      "iteration  20: log likelihood of observed labels = -412057.22479987\n",
      "iteration  30: log likelihood of observed labels = -412793.40379139\n",
      "iteration  40: log likelihood of observed labels = -412834.31989838\n",
      "iteration  50: log likelihood of observed labels = -412836.58802339\n",
      "iteration  60: log likelihood of observed labels = -412836.71373525\n",
      "iteration  70: log likelihood of observed labels = -412836.72070284\n",
      "iteration  80: log likelihood of observed labels = -412836.72108901\n",
      "iteration  90: log likelihood of observed labels = -412836.72111042\n",
      "iteration 100: log likelihood of observed labels = -412836.72111160\n",
      "iteration 200: log likelihood of observed labels = -412836.72111167\n",
      "iteration 300: log likelihood of observed labels = -412836.72111167\n",
      "iteration 400: log likelihood of observed labels = -412836.72111167\n",
      "iteration 500: log likelihood of observed labels = -412836.72111167\n",
      "iteration   0: log likelihood of observed labels = -35235.27023929\n",
      "iteration   1: log likelihood of observed labels = -102780.37368705\n",
      "iteration   2: log likelihood of observed labels = -229459.70479637\n",
      "iteration   3: log likelihood of observed labels = -180427.62084455\n",
      "iteration   4: log likelihood of observed labels = -229950.08860093\n",
      "iteration   5: log likelihood of observed labels = -180374.25351776\n",
      "iteration   6: log likelihood of observed labels = -229954.14786046\n",
      "iteration   7: log likelihood of observed labels = -180373.81157988\n",
      "iteration   8: log likelihood of observed labels = -229954.18146422\n",
      "iteration   9: log likelihood of observed labels = -180373.80792137\n",
      "iteration  10: log likelihood of observed labels = -229954.18174241\n",
      "iteration  11: log likelihood of observed labels = -180373.80789109\n",
      "iteration  12: log likelihood of observed labels = -229954.18174471\n",
      "iteration  13: log likelihood of observed labels = -180373.80789084\n",
      "iteration  14: log likelihood of observed labels = -229954.18174473\n",
      "iteration  15: log likelihood of observed labels = -180373.80789083\n",
      "iteration  20: log likelihood of observed labels = -229954.18174473\n",
      "iteration  30: log likelihood of observed labels = -229954.18174473\n",
      "iteration  40: log likelihood of observed labels = -229954.18174473\n",
      "iteration  50: log likelihood of observed labels = -229954.18174473\n",
      "iteration  60: log likelihood of observed labels = -229954.18174473\n",
      "iteration  70: log likelihood of observed labels = -229954.18174473\n",
      "iteration  80: log likelihood of observed labels = -229954.18174473\n",
      "iteration  90: log likelihood of observed labels = -229954.18174473\n",
      "iteration 100: log likelihood of observed labels = -229954.18174473\n"
     ]
    }
   ],
   "source": [
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=10, max_iter=501)\n",
    "coefficients_1e2_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e2, max_iter=501)\n",
    "coefficients_1e3_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e3, max_iter=501)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
