
import pandas as pd
import numpy as np
import json
import sklearn
import sklearn.ensemble
from sklearn.ensemble import GradientBoostingClassifier

loans = pd.read_csv('../../uw_ml/data/lending-club-data.csv')

# safe_loans =  1 => safe
# safe_loans = -1 => risky
loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)
loans = loans.drop('bad_loans', axis = 1)

target = 'safe_loans'
features = ['grade',                     # grade of the loan (categorical)
            'sub_grade_num',             # sub-grade of the loan as a number from 0 to 1
            'short_emp',                 # one year or less of employment
            'emp_length_num',            # number of years of employment
            'home_ownership',            # home_ownership status: own, mortgage or rent
            'dti',                       # debt to income ratio
            'purpose',                   # the purpose of the loan
            'payment_inc_ratio',         # ratio of the monthly payment to income
            'delinq_2yrs',               # number of delinquincies
             'delinq_2yrs_zero',          # no delinquincies in last 2 years
            'inq_last_6mths',            # number of creditor inquiries in last 6 months
            'last_delinq_none',          # has borrower had a delinquincy
            'last_major_derog_none',     # has borrower had 90 day or worse rating
            'open_acc',                  # number of open credit accounts
            'pub_rec',                   # number of derogatory public records
            'pub_rec_zero',              # no derogatory public records
            'revol_util',                # percent of available credit being used
            'total_rec_late_fee',        # total late fees received to day
            'int_rate',                  # interest rate of the loan
            'total_rec_int',             # interest received to date
            'annual_inc',                # annual income of borrower
            'funded_amnt',               # amount committed to the loan
            'funded_amnt_inv',           # amount committed by investors for the loan
            'installment',               # monthly payment owed by the borrower
           ]

loans = loans[[target] + features].dropna()

loans = pd.get_dummies(loans)


with open('../../uw_ml/data/module-8-assignment-1-train-idx.json', 'r') as f: # Reads the list of most frequent words
    train_idx = json.load(f)
with open('../../uw_ml/data/module-8-assignment-1-validation-idx.json', 'r') as f1: # Reads the list of most frequent words
    validation_idx = json.load(f1)

train_data = loans.iloc[train_idx]
validation_data = loans.iloc[validation_idx]

validation_safe_loans = validation_data[validation_data[target] == 1]
validation_risky_loans = validation_data[validation_data[target] == -1]

sample_validation_data_risky = validation_risky_loans[0:2]
sample_validation_data_safe = validation_safe_loans[0:2]

sample_validation_data = sample_validation_data_safe.append(sample_validation_data_risky)
sample_validation_data

model_5 = GradientBoostingClassifier(n_estimators=5, 
                                          max_depth=6)

X = train_data.drop('safe_loans',1)

np.any(np.isnan(X))

model_5.fit(X, train_data['safe_loans'])

model_5.predict(sample_validation_data.drop('safe_loans',1))

model_5.predict_proba(sample_validation_data.drop('safe_loans',1))

model_5.score(validation_data.drop('safe_loans',1), validation_data['safe_loans'])

predict_safeloans = model_5.predict(validation_data.drop('safe_loans',1))

predict_safeloans

fp = sum(predict_safeloans > validation_data['safe_loans'])
fp

# false negative
fn = sum(predict_safeloans < validation_data['safe_loans'])
fn

validation_data['predictions'] = model_5.predict_proba(validation_data.drop('safe_loans',1))[:,1]

assert sum((validation_data.predictions > 1) | (validation_data.predictions < 0)) == 0

validation_data[['grade_A','grade_B','grade_C','grade_D','predictions']].sort_values('predictions', ascending = False).head(5)

mistake_cost = (10000 * fn) + (20000 * fp)
mistake_cost

validation_data.drop(columns='predictions', axis=1, inplace=True)

models = {}
val_prediction_data = validation_data.drop('safe_loans', 1)
val_label_data = validation_data['safe_loans']
for estimator in [5, 10,50,100,200,500]:
    model = GradientBoostingClassifier(n_estimators=10,
                                     max_depth=6)
    model.fit(X, train_data['safe_loans'])
    predictions = model.predict(val_prediction_data)
    fn = sum(predictions < val_label_data)
    fp = sum(predictions > val_label_data)
    score = model.score(val_prediction_data, val_label_data)
    models['model_' + str(estimator)] = {
        'model': model,
        'fn': fn,
        'fp': fp,
        'score': round(score,6),
        'classification_error': (1 - score),
        'sample_predictions': model.predict(sample_validation_data.drop('safe_loans',1))
    }
    print('Done training model_'+ str(estimator))


# provided by assignment
import matplotlib.pyplot as plt
%matplotlib inline
def make_figure(dim, title, xlabel, ylabel, legend):
    plt.rcParams['figure.figsize'] = dim
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    if legend is not None:
        plt.legend(loc=legend, prop={'size':15})
    plt.rcParams.update({'font.size': 16})
    plt.tight_layout()

training_errors = []
val_errors = []
for k,v in models.items():
    print('VAL',k, v['score'])
    val_errors.append(v['classification_error'])
    model = v['model']
    score = model.score(X, train_data['safe_loans'])
    classification_error = 1 - score
    training_errors.append(1 - score)
    
#     print(k, classification_error)
    

plt.plot([5, 10, 50, 100, 200, 500], training_errors, linewidth=4.0, label='Training error')
plt.plot([5, 10, 50, 100, 200, 500], val_errors, linewidth=4.0, label='Validation error')

make_figure(dim=(10,5), title='Error vs number of trees',
            xlabel='Number of trees',
            ylabel='Classification error',
            legend='best')
